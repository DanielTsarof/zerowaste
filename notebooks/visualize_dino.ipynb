{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17nTIMqtlOSo"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/mhamilton723/FeatUp\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def closest_multiple(n, mult):\n",
        "    \"\"\"\n",
        "    Finds the closest multiple of `mult` less than or equal to `n`.\n",
        "    \"\"\"\n",
        "    return n - (n % mult)\n",
        "\n",
        "def transform(img_path, patch_size=14):\n",
        "    \"\"\"\n",
        "    Preprocess an image to tensor form.\n",
        "    - Resizes to a fixed size (e.g., 448x448).\n",
        "    - Normalizes using ImageNet mean and std.\n",
        "    \"\"\"\n",
        "    bgr = cv2.imread(img_path)\n",
        "    height, width, _ = bgr.shape\n",
        "    new_h = closest_multiple(height, patch_size)\n",
        "    new_w = closest_multiple(width, patch_size)\n",
        "    resized = cv2.resize(bgr, (448, 448))\n",
        "\n",
        "    rgb = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
        "    rgb /= 255.0\n",
        "\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "    rgb = (rgb - mean) / std\n",
        "\n",
        "    rgb_chw = rgb.transpose(2, 0, 1)  # (3, H, W)\n",
        "    tensor = torch.from_numpy(rgb_chw).unsqueeze(0)  # (1, 3, H, W)\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "2w0QggSilRIh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_featup_pca_images(img_path, upsampler, device=\"cpu\", patch_size=14):\n",
        "    \"\"\"\n",
        "    Applies PCA on feature maps to reduce dimensionality for visualization.\n",
        "    \"\"\"\n",
        "    # 1) Preprocess image\n",
        "    img_tensor = transform(img_path, patch_size=patch_size).to(device)\n",
        "\n",
        "    # 2) Generate feature maps with FeatUp model\n",
        "    with torch.no_grad():\n",
        "        feats_upsampled = upsampler(img_tensor)\n",
        "    B, C, H_up, W_up = feats_upsampled.shape\n",
        "\n",
        "    # 3) Perform PCA\n",
        "    feats_2d = feats_upsampled.permute(0, 2, 3, 1).reshape(-1, C).cpu().numpy()\n",
        "    pca = PCA(n_components=3)\n",
        "    feats_pca_3d = pca.fit_transform(feats_2d)\n",
        "    scaler = MinMaxScaler()\n",
        "    feats_pca_3d = scaler.fit_transform(feats_pca_3d)\n",
        "\n",
        "    # 4) Reshape PCA result back to image\n",
        "    feats_pca_3d = feats_pca_3d.reshape(H_up, W_up, 3)\n",
        "    feats_pca_255 = (feats_pca_3d * 255).astype(np.uint8)\n",
        "\n",
        "    # 5) Prepare original image for comparison\n",
        "    input_img = img_tensor[0].permute(1, 2, 0).cpu().numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "    std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
        "    inp = np.clip(input_img * std + mean, 0.0, 1.0)\n",
        "    inp_255 = (inp * 255).astype(np.uint8)\n",
        "\n",
        "    return inp_255, feats_pca_255"
      ],
      "metadata": {
        "id": "BbilLL7glTkG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_images_grid_featup(img_paths, upsampler, device=\"cpu\", patch_size=14):\n",
        "    \"\"\"\n",
        "    Visualizes input images and their PCA-transformed feature maps side by side.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for path in img_paths:\n",
        "        inp_255, feats_pca_255 = get_featup_pca_images(\n",
        "            path, upsampler, device=device, patch_size=patch_size\n",
        "        )\n",
        "        results.append((path, inp_255, feats_pca_255))\n",
        "\n",
        "    # Create a grid to display images\n",
        "    n_images = len(results)\n",
        "    fig, axes = plt.subplots(nrows=n_images, ncols=2, figsize=(10, 5 * n_images))\n",
        "\n",
        "    if n_images == 1:\n",
        "        axes = [axes]  # Handle case with a single image\n",
        "\n",
        "    for row_idx, (path, inp_img, feat_img) in enumerate(results):\n",
        "        ax_left, ax_right = axes[row_idx]\n",
        "        ax_left.imshow(inp_img)\n",
        "        ax_left.axis(\"off\")\n",
        "\n",
        "        ax_right.imshow(feat_img)\n",
        "        ax_right.set_title(\"FeatUp + PCA\")\n",
        "        ax_right.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "FpH__D0ylxmo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example image paths\n",
        "image_paths = [\n",
        "    \"/content/splits_final_deblurred/train/data/04_frame_036100.PNG\",\n",
        "    \"/content/splits_final_deblurred/train/data/04_frame_036200.PNG\",\n",
        "]\n",
        "\n",
        "upsampler = torch.hub.load(\"mhamilton723/FeatUp\", \"dinov2\", use_norm=False)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Visualize images and PCA-transformed features\n",
        "visualize_images_grid_featup(image_paths, upsampler, device=device, patch_size=14)"
      ],
      "metadata": {
        "id": "Fz6ZXx0Wly4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "daKEuEnJl9LJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}